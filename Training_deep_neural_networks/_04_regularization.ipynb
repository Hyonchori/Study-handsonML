{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "otherwise-sight",
   "metadata": {},
   "source": [
    "# 규제\n",
    "심층 신경망은 전형적으로 수만, 때로는 수백만 개의 파라미터를 갖고 있음. 이 때문에 네트워크의 자유도가 매우 높음.  \n",
    "덕분에 대규모의 복잡한 데이터셋을 학습할 수 있지만 **학습 데이터셋에 과대적합될 위험이 매우 크다는 것을 의미.**  \n",
    "**규제**가 필요하다. 앞서 **조기 종료, 배치 정규화**같은 것을 이미 다뤄봤음. 이번엔  \n",
    "- l1, l2 규제\n",
    "- 드롭아웃\n",
    "- 맥스-노름(max-norm) 규제\n",
    "\n",
    "를 알아보겠음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-columbia",
   "metadata": {},
   "source": [
    "---\n",
    "## l1, l2 규제\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "annual-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sustainable-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=keras.regularizers.l1(0.01))\n",
    "\n",
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=keras.regularizers.l1_l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-permission",
   "metadata": {},
   "source": [
    "**l2, l1, l1_l2** 함수들은 학습하는 동안 규제 손실을 계산하기 위해 각 스텝에서 호출되는 규제 객체를 반환함.  \n",
    "이 손실은 최종 손실에 합산됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "norman-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                          activation=\"elu\",\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "operating-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\",\n",
    "                    kernel_initializer=\"glorot_uniform\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-sullivan",
   "metadata": {},
   "source": [
    "일반적으로 네트워크의 모든 은닉층에 동일한 활성함수, 동일한 초기화 전략을 사용하거나 모든 층에 동일한 규제를 적용함.  \n",
    "이는 코드를 고치기 어렵게 만듦.  \n",
    "이를 피하기 위해 파이썬의 **functools.partial()** 함수를 사용하여 기본 매개변수 값을 사용하여 함수 호출을 감쌀 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-preliminary",
   "metadata": {},
   "source": [
    "---\n",
    "## 드롭아웃\n",
    "**Dropout**은 심층 신경망에서 가장 인기있는 규제 기법 중 하나임.  \n",
    "최고 성능을 내는 신경망조차도 드롭 아웃을 적용해서 정확도를 1~2% 높일 수 있음.  \n",
    ">매 학습 스텝에서 각 뉴런(입력 뉴런은 포함, 출력 뉴런은 제외)은 임시적으로 드롭아웃될 확률 p를 가짐.  \n",
    "**드롭아웃 비율** p는 보통 10~50%로 설정. 순환 신경망에선 20~30%, 합성곱 신경망에선 40~50%에 가까움.  \n",
    "학습이 끝나면 드롭아웃은 작동하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-challenge",
   "metadata": {},
   "source": [
    "### 드롭아웃 효과\n",
    "- 드롭아웃으로 학습된 뉴런은 이웃 뉴런에 적응될 수 없어서 자기 자신이 유용한 뉴런이 되야함.\n",
    "- 몇 개의 입력 뉴런에만 지나치게 의존할 수 없어서 모든 입력 뉴런에 주의를 기울임. 즉 입력값의 작은 변화에 덜 민감해짐(일반화 성능 업)\n",
    "\n",
    "드롭아웃의 능력을 이해하는 또 다른 방법은 각 학습 스텝마다 고유한 네트워크가 생성된다고 보면 됨.  \n",
    "10000번 학습을 진행하면 10000개의 다른 신경망이 생성되는 것인데 이 신경망은 대부분의 가중치를 공유하고 있어 완전 독립되지 않음.  \n",
    "하지만 그럼에도 모두 다름. 결과적으로 최종 신경망은 이 모든 신경망들을 평균한 **앙상블**로 볼 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-exhibition",
   "metadata": {},
   "source": [
    "### 드롭아웃 주의점\n",
    "만약 p=50%로 설정하면 학습이 아닌 테스트동안엔 하나의 뉴런이 학습때보다 (평균적으로) 2배 많은 입력 뉴런과 연결됨.  \n",
    "이렇게 되면 각 뉴런이 학습한 것보다 거의 두 배 많은 입력 신호를 받아서 잘 동작하지 않을 것임.  \n",
    "이런 점을 보상하기 위해 학습 후에 각 뉴런의 **연결 가중치에 0.5를 곱할 필요가 있음**  \n",
    ">일반적으로 말하면 학습이 끝나면 각 뉴런의 연결 가중치에 **보존 확률 (1-p)를 곱해야 함**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exciting-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-dispute",
   "metadata": {},
   "source": [
    "- **모델이 과대적합되었다면** : 드롭아웃 비율 증가\n",
    "- **모델이 과소적합되었담녀** : 드롭아웃 비율 감소\n",
    "\n",
    "> 또한 많은 최신 신경망 구조에서는 **마지막 은닉층 뒤에만 드롭아웃을 사용한다고 함**\n",
    "\n",
    ">드롭 아웃은 **수렴을 상당히 느리게 만드는 경향이 있지만** 적절하게 튜닝하면 **훨씬 좋은 모델을 만듦**  \n",
    "따라서 일반적으로 추가적인 시간과 노력을 기울일 가치가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-prescription",
   "metadata": {},
   "source": [
    "> 드롭아웃은 학습하는 동안에만 활성화되므로 **학습 손실과 검증 손실을 비교하면 안됨**  \n",
    "특히 비슷한 학습 손실과 검증 손실을 얻었더라도 모델이 학습 데이터셋에 과대적합되었을 수 있음.  \n",
    "따라서 **드롭 아웃을 빼고(예를 들어 학습이 끝나고) 학습 손실을 평가해야 함**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-porcelain",
   "metadata": {},
   "source": [
    "---\n",
    "## 몬테 카를로 드롭아웃\n",
    "야린 갤, 주빈 가라마니의 2016년 논문에서 **몬테 카를로 드롭아웃**이라는 기법을 소개함.  \n",
    ">몬테 카를로 드롭아웃은 학습된 드롭아웃 모델을 재학습하거나 전혀 수정하지 않고 성능을 크게 향상시킬 수 있다고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "immediate-pierre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_val, x_train = x_train_full[: 5000]/255.0, x_train_full[5000: ]/255.0\n",
    "y_val, y_train = y_train_full[: 5000], y_train_full[5000: ]\n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "excess-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_probs = np.stack([model(x_test, training=True) for sample in range(100)])\n",
    "y_proba = y_probs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "operational-register",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 10000, 10), (10000, 10))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probs.shape, y_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "complex-novel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-kernel",
   "metadata": {},
   "source": [
    "우선 **training=True**로 설정하여 드롭아웃을 활성화시키고 테스트셋에서 100번의 예측을 만듦.  \n",
    "드롭아웃이 활성화되었기 때문에 이 100개의 예측은 모두 다름.  \n",
    "이 100개의 예측을 **axis=0**으로 평균하여 원래 테스트셋에 대해 **predict**한 것과 동일한 크기의 배열을 얻게 됨.\n",
    "> 이게 끝임. 즉 드롭아웃으로 만든 예측을 평균하면 일반적으로 **드롭아웃 없이 예측한 하나의 결과보다 안정적임**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "found-melissa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 1.0069 - accuracy: 0.6428 - val_loss: 0.5146 - val_accuracy: 0.8248\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6049 - accuracy: 0.7819 - val_loss: 0.4666 - val_accuracy: 0.8356\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5469 - accuracy: 0.8017 - val_loss: 0.4446 - val_accuracy: 0.8440\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5225 - accuracy: 0.8105 - val_loss: 0.4247 - val_accuracy: 0.8504\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5070 - accuracy: 0.8142 - val_loss: 0.4082 - val_accuracy: 0.8574\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4877 - accuracy: 0.8231 - val_loss: 0.4002 - val_accuracy: 0.8624\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4871 - accuracy: 0.8235 - val_loss: 0.3960 - val_accuracy: 0.8656\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4697 - accuracy: 0.8310 - val_loss: 0.3934 - val_accuracy: 0.8586\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4654 - accuracy: 0.8279 - val_loss: 0.3814 - val_accuracy: 0.8670\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4580 - accuracy: 0.8319 - val_loss: 0.3862 - val_accuracy: 0.8628\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4561 - accuracy: 0.8339 - val_loss: 0.3760 - val_accuracy: 0.8648\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4448 - accuracy: 0.8374 - val_loss: 0.3770 - val_accuracy: 0.8650\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4441 - accuracy: 0.8371 - val_loss: 0.3680 - val_accuracy: 0.8712\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4382 - accuracy: 0.8407 - val_loss: 0.3651 - val_accuracy: 0.8694\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4320 - accuracy: 0.8398 - val_loss: 0.3585 - val_accuracy: 0.8726\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4289 - accuracy: 0.8428 - val_loss: 0.3575 - val_accuracy: 0.8700\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4181 - accuracy: 0.8471 - val_loss: 0.3572 - val_accuracy: 0.8734\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4224 - accuracy: 0.8451 - val_loss: 0.3567 - val_accuracy: 0.8720\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4216 - accuracy: 0.8457 - val_loss: 0.3501 - val_accuracy: 0.8738\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4161 - accuracy: 0.8458 - val_loss: 0.3461 - val_accuracy: 0.8776\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4123 - accuracy: 0.8468 - val_loss: 0.3479 - val_accuracy: 0.8750\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4160 - accuracy: 0.8483 - val_loss: 0.3448 - val_accuracy: 0.8770\n",
      "Epoch 23/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4099 - accuracy: 0.8477 - val_loss: 0.3466 - val_accuracy: 0.8740\n",
      "Epoch 24/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3979 - accuracy: 0.8510 - val_loss: 0.3420 - val_accuracy: 0.8806\n",
      "Epoch 25/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4062 - accuracy: 0.8501 - val_loss: 0.3400 - val_accuracy: 0.8784\n",
      "Epoch 26/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4042 - accuracy: 0.8505 - val_loss: 0.3398 - val_accuracy: 0.8752\n",
      "Epoch 27/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3988 - accuracy: 0.8507 - val_loss: 0.3332 - val_accuracy: 0.8816\n",
      "Epoch 28/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3947 - accuracy: 0.8559 - val_loss: 0.3361 - val_accuracy: 0.8810\n",
      "Epoch 29/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3983 - accuracy: 0.8547 - val_loss: 0.3376 - val_accuracy: 0.8776\n",
      "Epoch 30/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3989 - accuracy: 0.8534 - val_loss: 0.3324 - val_accuracy: 0.8806\n",
      "Epoch 31/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3897 - accuracy: 0.8562 - val_loss: 0.3283 - val_accuracy: 0.8820\n",
      "Epoch 32/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3846 - accuracy: 0.8561 - val_loss: 0.3281 - val_accuracy: 0.8818\n",
      "Epoch 33/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3914 - accuracy: 0.8543 - val_loss: 0.3295 - val_accuracy: 0.8826\n",
      "Epoch 34/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3917 - accuracy: 0.8565 - val_loss: 0.3247 - val_accuracy: 0.8812\n",
      "Epoch 35/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3861 - accuracy: 0.8564 - val_loss: 0.3273 - val_accuracy: 0.8798\n",
      "Epoch 36/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3785 - accuracy: 0.8600 - val_loss: 0.3237 - val_accuracy: 0.8814\n",
      "Epoch 37/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3772 - accuracy: 0.8610 - val_loss: 0.3275 - val_accuracy: 0.8814\n",
      "Epoch 38/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3784 - accuracy: 0.8599 - val_loss: 0.3208 - val_accuracy: 0.8856\n",
      "Epoch 39/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3786 - accuracy: 0.8598 - val_loss: 0.3223 - val_accuracy: 0.8820\n",
      "Epoch 40/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3712 - accuracy: 0.8607 - val_loss: 0.3190 - val_accuracy: 0.8836\n",
      "Epoch 41/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3772 - accuracy: 0.8596 - val_loss: 0.3178 - val_accuracy: 0.8836\n",
      "Epoch 42/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3700 - accuracy: 0.8624 - val_loss: 0.3172 - val_accuracy: 0.8848\n",
      "Epoch 43/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3791 - accuracy: 0.8595 - val_loss: 0.3236 - val_accuracy: 0.8824\n",
      "Epoch 44/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3688 - accuracy: 0.8610 - val_loss: 0.3189 - val_accuracy: 0.8832\n",
      "Epoch 45/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3699 - accuracy: 0.8625 - val_loss: 0.3179 - val_accuracy: 0.8810\n",
      "Epoch 46/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3722 - accuracy: 0.8627 - val_loss: 0.3130 - val_accuracy: 0.8846\n",
      "Epoch 47/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3747 - accuracy: 0.8619 - val_loss: 0.3125 - val_accuracy: 0.8844\n",
      "Epoch 48/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3623 - accuracy: 0.8635 - val_loss: 0.3150 - val_accuracy: 0.8822\n",
      "Epoch 49/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3696 - accuracy: 0.8642 - val_loss: 0.3138 - val_accuracy: 0.8838\n",
      "Epoch 50/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3660 - accuracy: 0.8648 - val_loss: 0.3142 - val_accuracy: 0.8836\n",
      "Epoch 51/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3703 - accuracy: 0.8628 - val_loss: 0.3141 - val_accuracy: 0.8864\n",
      "Epoch 52/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3634 - accuracy: 0.8645 - val_loss: 0.3142 - val_accuracy: 0.8820\n",
      "Epoch 53/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3667 - accuracy: 0.8645 - val_loss: 0.3134 - val_accuracy: 0.8838\n",
      "Epoch 54/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3606 - accuracy: 0.8642 - val_loss: 0.3071 - val_accuracy: 0.8858\n",
      "Epoch 55/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3587 - accuracy: 0.8665 - val_loss: 0.3072 - val_accuracy: 0.8864\n",
      "Epoch 56/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3666 - accuracy: 0.8648 - val_loss: 0.3140 - val_accuracy: 0.8850\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3575 - accuracy: 0.8666 - val_loss: 0.3102 - val_accuracy: 0.8858\n",
      "Epoch 58/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3554 - accuracy: 0.8674 - val_loss: 0.3053 - val_accuracy: 0.8898\n",
      "Epoch 59/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3596 - accuracy: 0.8689 - val_loss: 0.3070 - val_accuracy: 0.8866\n",
      "Epoch 60/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3584 - accuracy: 0.8675 - val_loss: 0.3089 - val_accuracy: 0.8856\n",
      "Epoch 61/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3554 - accuracy: 0.8675 - val_loss: 0.3053 - val_accuracy: 0.8874\n",
      "Epoch 62/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3477 - accuracy: 0.8701 - val_loss: 0.3030 - val_accuracy: 0.8876\n",
      "Epoch 63/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3536 - accuracy: 0.8685 - val_loss: 0.3043 - val_accuracy: 0.8878\n",
      "Epoch 64/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3470 - accuracy: 0.8696 - val_loss: 0.3065 - val_accuracy: 0.8854\n",
      "Epoch 65/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3513 - accuracy: 0.8687 - val_loss: 0.3060 - val_accuracy: 0.8874\n",
      "Epoch 66/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3604 - accuracy: 0.8649 - val_loss: 0.3050 - val_accuracy: 0.8822\n",
      "Epoch 67/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3510 - accuracy: 0.8674 - val_loss: 0.3044 - val_accuracy: 0.8882\n",
      "Epoch 68/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3540 - accuracy: 0.8671 - val_loss: 0.3034 - val_accuracy: 0.8884\n",
      "Epoch 69/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3457 - accuracy: 0.8709 - val_loss: 0.2986 - val_accuracy: 0.8910\n",
      "Epoch 70/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3534 - accuracy: 0.8688 - val_loss: 0.3025 - val_accuracy: 0.8854\n",
      "Epoch 71/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3449 - accuracy: 0.8719 - val_loss: 0.2989 - val_accuracy: 0.8904\n",
      "Epoch 72/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3453 - accuracy: 0.8714 - val_loss: 0.3005 - val_accuracy: 0.8876\n",
      "Epoch 73/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3539 - accuracy: 0.8691 - val_loss: 0.3009 - val_accuracy: 0.8866\n",
      "Epoch 74/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3415 - accuracy: 0.8711 - val_loss: 0.3038 - val_accuracy: 0.8866\n",
      "Epoch 75/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3495 - accuracy: 0.8702 - val_loss: 0.3003 - val_accuracy: 0.8888\n",
      "Epoch 76/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3369 - accuracy: 0.8754 - val_loss: 0.3010 - val_accuracy: 0.8888\n",
      "Epoch 77/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3446 - accuracy: 0.8723 - val_loss: 0.3001 - val_accuracy: 0.8902\n",
      "Epoch 78/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3458 - accuracy: 0.8702 - val_loss: 0.2954 - val_accuracy: 0.8908\n",
      "Epoch 79/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3438 - accuracy: 0.8702 - val_loss: 0.2959 - val_accuracy: 0.8890\n",
      "Epoch 80/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3436 - accuracy: 0.8697 - val_loss: 0.2948 - val_accuracy: 0.8894\n",
      "Epoch 81/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3416 - accuracy: 0.8725 - val_loss: 0.2973 - val_accuracy: 0.8876\n",
      "Epoch 82/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3416 - accuracy: 0.8700 - val_loss: 0.2949 - val_accuracy: 0.8918\n",
      "Epoch 83/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3381 - accuracy: 0.8733 - val_loss: 0.2930 - val_accuracy: 0.8904\n",
      "Epoch 84/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3397 - accuracy: 0.8738 - val_loss: 0.3013 - val_accuracy: 0.8896\n",
      "Epoch 85/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3446 - accuracy: 0.8689 - val_loss: 0.2946 - val_accuracy: 0.8876\n",
      "Epoch 86/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3371 - accuracy: 0.8732 - val_loss: 0.2953 - val_accuracy: 0.8910\n",
      "Epoch 87/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3375 - accuracy: 0.8729 - val_loss: 0.2944 - val_accuracy: 0.8898\n",
      "Epoch 88/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3382 - accuracy: 0.8741 - val_loss: 0.2936 - val_accuracy: 0.8904\n",
      "Epoch 89/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3367 - accuracy: 0.8749 - val_loss: 0.2939 - val_accuracy: 0.8886\n",
      "Epoch 90/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3343 - accuracy: 0.8745 - val_loss: 0.2938 - val_accuracy: 0.8896\n",
      "Epoch 91/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3340 - accuracy: 0.8763 - val_loss: 0.2945 - val_accuracy: 0.8898\n",
      "Epoch 92/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3403 - accuracy: 0.8728 - val_loss: 0.2977 - val_accuracy: 0.8896\n",
      "Epoch 93/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3350 - accuracy: 0.8766 - val_loss: 0.2915 - val_accuracy: 0.8936\n",
      "Epoch 94/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3371 - accuracy: 0.8736 - val_loss: 0.2889 - val_accuracy: 0.8918\n",
      "Epoch 95/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3317 - accuracy: 0.8783 - val_loss: 0.2891 - val_accuracy: 0.8924\n",
      "Epoch 96/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3388 - accuracy: 0.8723 - val_loss: 0.2913 - val_accuracy: 0.8914\n",
      "Epoch 97/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3313 - accuracy: 0.8739 - val_loss: 0.2886 - val_accuracy: 0.8938\n",
      "Epoch 98/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3300 - accuracy: 0.8781 - val_loss: 0.2909 - val_accuracy: 0.8904\n",
      "Epoch 99/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3325 - accuracy: 0.8762 - val_loss: 0.2903 - val_accuracy: 0.8932\n",
      "Epoch 100/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3342 - accuracy: 0.8748 - val_loss: 0.2921 - val_accuracy: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x279df91ab20>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=\"sgd\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=100,\n",
    "             validation_data=(x_val, y_val),\n",
    "             callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "polished-acceptance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.05, 0.  , 0.94]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(x_test[:1]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "incorrect-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = np.stack([model(x_test, training=True) for sample in range(100)])\n",
    "y_proba = y_probs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "british-peter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.02, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.03, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 0.  , 0.02, 0.  , 0.66]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.07, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.88]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.55, 0.  , 0.43]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.07, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.21, 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.02, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.06, 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.12, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.27, 0.  , 0.7 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.05, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.31, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.05, 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.13, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.41, 0.  , 0.08, 0.  , 0.51]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.07, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.05, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.19, 0.  , 0.33, 0.  , 0.48]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.05, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.09, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.02, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.04, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.03, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.24, 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.09, 0.  , 0.88]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.02, 0.  , 0.83]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.18, 0.  , 0.8 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.11, 0.  , 0.88]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.23, 0.  , 0.04, 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.08, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.14, 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.27, 0.  , 0.06, 0.  , 0.67]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.66, 0.  , 0.05, 0.  , 0.29]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.01, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.07, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.09, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.04, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.17, 0.  , 0.76]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.44, 0.  , 0.46]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.02, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.15, 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.13, 0.  , 0.84]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.02, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.21, 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.08, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.05, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.03, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.01, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.09, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.04, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.04, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.04, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.01, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.26, 0.  , 0.33, 0.  , 0.42]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.02, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.12, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.08, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.14, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.17, 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.21, 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.38, 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.17, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.04, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.19, 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.04, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.03, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.06, 0.  , 0.72]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.06, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.14, 0.  , 0.8 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_probs[:, :1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "decreased-webster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.09, 0.  , 0.86]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-horse",
   "metadata": {},
   "source": [
    "확실히 predict로 출력한 확률보다 드롭아웃들의 평균이 출력한 확률이 더 그럴듯 함.  \n",
    "이 확률 추정의 표준 분포도 확인해볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "contemporary-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.1 , 0.  , 0.14]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probs.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "impossible-romance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8777"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "acc_pred = np.sum(y_pred == y_test) / len(y_test)\n",
    "acc_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "overall-burning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8784"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_dropout = np.argmax(y_proba, axis=1)\n",
    "acc_pred_dropout = np.sum(y_pred_dropout == y_test) / len(y_test)\n",
    "acc_pred_dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-berry",
   "metadata": {},
   "source": [
    "오 그리고 모델의 정확도도 아주 조금 상향됨 ㅎㅎ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-neutral",
   "metadata": {},
   "source": [
    "> 하지만 **학습하는 동안 다르게 작동하는(배치 정규화같은) 층을 갖고 있다면** 이와 같이 학습 모드를 강제로 설정해서는 안됨.  \n",
    "대신 Dropout층을 다음과 같은 **MCDropout** 클래스를 정의하여 바꿔주면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "harmful-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-prompt",
   "metadata": {},
   "source": [
    "즉 **training**매개변수를 강제로 True로 설정하여 학습이 끝나도 드롭아웃이 활성화되도록 하는 것임.\n",
    ">**MC드롭아웃은 모델의 성능을 높여주고 더 정확한 불확실성 추정을 제공**하는 좋은 기술임.  \n",
    "그리고 학습하는 동안은 일반적인 드롭아웃처럼 수행하여 규제처럼 작용하기도 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-payment",
   "metadata": {},
   "source": [
    "---\n",
    "## 맥스-노름 규제\n",
    "> 이 방식은 각각의 뉴런에 대해 입력의 **연결 가중지 w의 l2 노름이 일정 값(r)을 넘지 않도록 제한함**\n",
    "\n",
    "맥스-노름 규제는 전체 손실 함수에 규제 손실 항을 추가하지 않음.  \n",
    "대신 일반적으로 매 훈련 스텝이 끝나고 w의 l2노름을 계산하고 필요하면 w의 스케일을 조정함.  \n",
    "- **r을 줄이면** 규제의 양이 증가하여 과대적합을 감소시키는 데 도움이 됨.  \n",
    "\n",
    ">맥스-노름 규제는 (배치 정규화를 사용하지 않았을 때) 불안정한 그레디언트 문제를 완화하는 데 도움을 줄 수 있음  \n",
    "(그럼 배치정규화 쓰면 안써도 된다는 뜻??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "spatial-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                          kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-ordinance",
   "metadata": {},
   "source": [
    "매 이터레이션마다 모델의 fit()메서드가 층의 가중치와 함께 max_norm()이 반환한 객체를 호출하고 스케일이 조정된 가중치를 반환받음.  \n",
    "이 값을 사용하여 층의 가중치를 바꿈.  \n",
    "사용자 정의 규제 함수를 정의하여 kernel_constraint 매개변수에 지정하여 편향을 규제할 수도 있음.  \n",
    ">기본적으로 0으로 설정된 **axis** 매개변수가 있음  \n",
    "**합성곱 신경망에 사용하려면** axis=[0, 1, 2] 이런식으로 적절하게 지정해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-meter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
