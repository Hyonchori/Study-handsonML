{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hollow-assault",
   "metadata": {},
   "source": [
    "# 2.1 사전 학습된 층 재사용하기\n",
    "일반적으로 아주 큰 규모의 DNN을 처음부터 새로 학습하는 것은 좋은 생각이 아님.  \n",
    "해결하려는 것과 비슷한 유형의 문제를 처리한 신경망이 이미 있는지 찾아본 다음, 그 신경망의 하위층(입력 부분)을 재사용하는 것이 좋음.  \n",
    "이를 **전이 학습** 이라고 함.  \n",
    "> 전이 학습은 학습 속도를 크게 높일 뿐만 아니라 필요한 학습 데이터도 크게 줄여줌\n",
    "\n",
    "예를 들어 동물, 식물, 자동차를 포함한 100개의 카테고리로 구분된 이미지를 분류하도록 학습한 DNN이 있을 때, 자동차의 종류를 분류하는 DNN을 학습하려고 한다고 하자.  \n",
    "두 작업은 비슷한 점이 많고 심지어 일부 겹치기도 하므로 첫 번째 신경망의 일부를 재사용하면 좋음.  \n",
    "> 만약 원래 문제에서 사용한 것과 크기가 다른 이미지를 입력으로 사용한다면 원본 모델에 맞는 크기로 변경하는 전처리 단계 추가해야 함.\n",
    "> **일반적으로 전이 학습은 저수준 특성이 비슷한 입력에서 잘 작동함.** (즉 입력 부분이 비슷할 때)\n",
    "\n",
    "보통 원본 모델의 출력층을 바꿈. 이 층이 새로운 작업에 가장 유용하지 않는 층이고 새로운 작업에 필요한 출력 개수와 맞지 않을 수 있음.  \n",
    "비슷하게 원본 모델의 상위 은닉층(출력 부분)은 하위 은닉층(입력 부분)보다 덜 유용함. 새로운 작업에 유용한 고수준 특성은 원본 작업에서 유용했던 특성과 상당히 다르기 때문.  \n",
    "> **따라서 재사용할 층 개수를 잘 선정하는것이 필요**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-daniel",
   "metadata": {},
   "source": [
    "---\n",
    "### 전이 학습 사용\n",
    "> - 먼저 재사용하는 층을 모두 동결 (경사 하강법으로 가중치가 바뀌지 않도록)\n",
    ">- 그 다음 모델을 학습하고 성능을 평가\n",
    ">- 맨 위에 있는 한 두개의 은닉층의 동결을 해제하고 다시 학습하여 성능이 향상되는지 확인\n",
    ">- 학습 데이터가 많을수록 많은 층의 동결을 해제할 수 있음.\n",
    "\n",
    "> 재사용 층의 동결을 해제할 때는 학습률을 줄이는 것이 좋음.\n",
    "\n",
    ">- 만약 여전히 좋은 성능이 안 나오고 학습 데이터도 적다면 상위 은닉층(들)을 제거하고 남은 은닉층을 다시 동결.\n",
    ">- 이런 식으로 재사용할 은닉층의 적절한 개수를 찾을 때까지 반복.\n",
    "\n",
    ">- 학습 데이터가 아주 많다면 은닉층을 제거하는 대신 다른 것으로 바꾸거나 더 많은 은닉층을 추가할 수도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-missouri",
   "metadata": {},
   "source": [
    "---\n",
    "### 전이 학습 주의점\n",
    "전이 학습은 풀리 커넥티드 네트워크에서는 잘 동작하지 않음. 특정 패턴을 학습하기 때문일 것.  \n",
    "> 전이 학습은 대체로 일반적인 특성을(특히 아래쪽 층에서) 감지하는 경향이 있는 **심층 합성곱 신경망** 에서 잘 동작함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-evolution",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.2 비지도 사전 학습\n",
    "레이블된 학습 데이터가 많지 않고, 문제와 비슷한 작업에 대해 학습된 모델을 찾을 수도 없을 때,  \n",
    "- 먼저 더 많은 레이블된 학습 데이터를 모아보거나\n",
    "- **비지도 사전학습 (unsupervised pretraining)** 을 수행할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-today",
   "metadata": {},
   "source": [
    "2010년까지만 해도 **제한된 볼츠만 머신 (RBM: restricted Boltzmann machine)** 을 사용한 비지도 사전 학습이 심층 신경망의 표준이었음  \n",
    "요즘엔 **GAN, 오토인코더** 를 많이 씀. 나중에 다룰 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-steering",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.3 보조 작업에서 사전 학습\n",
    "레이블된 학습 데이터가 많지 않다면 마지막 선택 사항은 레이블된 학습 데이터를 쉽게 얻거나 생성할 수 있는 보조 작업에서 첫 번째 신경망을 학습하는 것임. 그리고 이 신경망의 하위층(입력 부분)을 실제 작업을 위해 재사용함.  \n",
    "> 즉 첫 번째 신경망의 하위층이 두 번째 신경망에 재사용될 특성 추출기가 되는 것.\n",
    "\n",
    "예를 들어 얼굴 인식 시스템을 만들려고 할 때 개인별 이미지가 얼마 없을 경우.  \n",
    "- 인터넷에서 무작위로 많은 인물의 이미지를 수집해서 두 개의 다른 이미지가 같은 사람의 것인기 감지하는 첫 번째 신경망 학습\n",
    "- 이 신경망의 하위층을 재사용해 적은 양의 학습 데이터에서 얼굴을 잘 구분하는 분류기 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-tomato",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
