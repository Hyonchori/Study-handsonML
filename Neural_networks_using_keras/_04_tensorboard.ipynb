{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hundred-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "structured-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "dir_name = \"my_logs\"\n",
    "dir_path = os.path.join(os.curdir, dir_name)\n",
    "if os.path.exists(dir_path):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(dir_path)\n",
    "    \n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(dir_path, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "negative-affiliate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\my_logs\\\\run_2021_03_01-05_55_23'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "explicit-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "reasonable-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=x_train.shape[1: ])\n",
    "h1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "h2 = keras.layers.Dense(30, activation=\"relu\")(h1)\n",
    "concat = keras.layers.Concatenate()([input_, h2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tired-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "clear-teens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6225 - val_loss: 9.5640\n",
      "Epoch 2/200\n",
      "363/363 [==============================] - 0s 536us/step - loss: 0.8047 - val_loss: 37.7356\n",
      "Epoch 3/200\n",
      "363/363 [==============================] - 0s 533us/step - loss: 5.7527 - val_loss: 1.2236\n",
      "Epoch 4/200\n",
      "363/363 [==============================] - 0s 519us/step - loss: 0.6097 - val_loss: 0.5462\n",
      "Epoch 5/200\n",
      "363/363 [==============================] - 0s 547us/step - loss: 0.4570 - val_loss: 0.4811\n",
      "Epoch 6/200\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.4424 - val_loss: 0.4708\n",
      "Epoch 7/200\n",
      "363/363 [==============================] - 0s 528us/step - loss: 0.4471 - val_loss: 0.4608\n",
      "Epoch 8/200\n",
      "363/363 [==============================] - 0s 528us/step - loss: 0.4234 - val_loss: 0.4546\n",
      "Epoch 9/200\n",
      "363/363 [==============================] - 0s 519us/step - loss: 0.4104 - val_loss: 0.4464\n",
      "Epoch 10/200\n",
      "363/363 [==============================] - 0s 514us/step - loss: 0.4142 - val_loss: 0.4408\n",
      "Epoch 11/200\n",
      "363/363 [==============================] - 0s 511us/step - loss: 0.4050 - val_loss: 0.4367\n",
      "Epoch 12/200\n",
      "363/363 [==============================] - 0s 506us/step - loss: 0.4008 - val_loss: 0.4328\n",
      "Epoch 13/200\n",
      "363/363 [==============================] - 0s 528us/step - loss: 0.3787 - val_loss: 0.4289\n",
      "Epoch 14/200\n",
      "363/363 [==============================] - 0s 522us/step - loss: 0.3868 - val_loss: 0.4260\n",
      "Epoch 15/200\n",
      "363/363 [==============================] - 0s 514us/step - loss: 0.3766 - val_loss: 0.4223\n",
      "Epoch 16/200\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.3787 - val_loss: 0.4191\n",
      "Epoch 17/200\n",
      "363/363 [==============================] - 0s 519us/step - loss: 0.3682 - val_loss: 0.4194\n",
      "Epoch 18/200\n",
      "363/363 [==============================] - 0s 517us/step - loss: 0.3689 - val_loss: 0.4142\n",
      "Epoch 19/200\n",
      "363/363 [==============================] - 0s 514us/step - loss: 0.3637 - val_loss: 0.4129\n",
      "Epoch 20/200\n",
      "363/363 [==============================] - 0s 522us/step - loss: 0.3655 - val_loss: 0.4112\n",
      "Epoch 21/200\n",
      "363/363 [==============================] - 0s 511us/step - loss: 0.3692 - val_loss: 0.4083\n",
      "Epoch 22/200\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.3399 - val_loss: 0.4082\n",
      "Epoch 23/200\n",
      "363/363 [==============================] - 0s 555us/step - loss: 0.3702 - val_loss: 0.4050\n",
      "Epoch 24/200\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.3635 - val_loss: 0.4036\n",
      "Epoch 25/200\n",
      "363/363 [==============================] - 0s 589us/step - loss: 0.3490 - val_loss: 0.4041\n",
      "Epoch 26/200\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.3458 - val_loss: 0.4017\n",
      "Epoch 27/200\n",
      "363/363 [==============================] - 0s 547us/step - loss: 0.3510 - val_loss: 0.4004\n",
      "Epoch 28/200\n",
      "363/363 [==============================] - 0s 580us/step - loss: 0.3477 - val_loss: 0.3998\n",
      "Epoch 29/200\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.3468 - val_loss: 0.3986\n",
      "Epoch 30/200\n",
      "363/363 [==============================] - 0s 684us/step - loss: 0.3529 - val_loss: 0.3979\n",
      "Epoch 31/200\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.3485 - val_loss: 0.3974\n",
      "Epoch 32/200\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.3461 - val_loss: 0.3964\n",
      "Epoch 33/200\n",
      "363/363 [==============================] - 0s 614us/step - loss: 0.3518 - val_loss: 0.3949\n",
      "Epoch 34/200\n",
      "363/363 [==============================] - 0s 574us/step - loss: 0.3469 - val_loss: 0.3937\n",
      "Epoch 35/200\n",
      "363/363 [==============================] - 0s 543us/step - loss: 0.3506 - val_loss: 0.3924\n",
      "Epoch 36/200\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.3352 - val_loss: 0.3944\n",
      "Epoch 37/200\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3421 - val_loss: 0.3917\n",
      "Epoch 38/200\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.3339 - val_loss: 0.3922\n",
      "Epoch 39/200\n",
      "363/363 [==============================] - 0s 508us/step - loss: 0.3316 - val_loss: 0.3909\n",
      "Epoch 40/200\n",
      "363/363 [==============================] - 0s 517us/step - loss: 0.3365 - val_loss: 0.3911\n",
      "Epoch 41/200\n",
      "363/363 [==============================] - 0s 517us/step - loss: 0.3285 - val_loss: 0.3893\n",
      "Epoch 42/200\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3402 - val_loss: 0.3881\n",
      "Epoch 43/200\n",
      "363/363 [==============================] - 0s 542us/step - loss: 0.3271 - val_loss: 0.3889\n",
      "Epoch 44/200\n",
      "363/363 [==============================] - 0s 522us/step - loss: 0.3394 - val_loss: 0.3884\n",
      "Epoch 45/200\n",
      "363/363 [==============================] - 0s 519us/step - loss: 0.3436 - val_loss: 0.3877\n",
      "Epoch 46/200\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.3212 - val_loss: 0.3868\n",
      "Epoch 47/200\n",
      "363/363 [==============================] - 0s 542us/step - loss: 0.3421 - val_loss: 0.3863\n",
      "Epoch 48/200\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.3381 - val_loss: 0.3852\n",
      "Epoch 49/200\n",
      "363/363 [==============================] - 0s 548us/step - loss: 0.3414 - val_loss: 0.3846\n",
      "Epoch 50/200\n",
      "363/363 [==============================] - 0s 519us/step - loss: 0.3244 - val_loss: 0.3841\n",
      "Epoch 51/200\n",
      "363/363 [==============================] - 0s 528us/step - loss: 0.3284 - val_loss: 0.3856\n",
      "Epoch 52/200\n",
      "363/363 [==============================] - 0s 522us/step - loss: 0.3351 - val_loss: 0.3838\n",
      "Epoch 53/200\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.3250 - val_loss: 0.3825\n",
      "Epoch 54/200\n",
      "363/363 [==============================] - 0s 517us/step - loss: 0.3255 - val_loss: 0.3832\n",
      "Epoch 55/200\n",
      "363/363 [==============================] - 0s 536us/step - loss: 0.3176 - val_loss: 0.3823\n",
      "Epoch 56/200\n",
      "363/363 [==============================] - 0s 531us/step - loss: 0.3083 - val_loss: 0.3818\n",
      "Epoch 57/200\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3170 - val_loss: 0.3815\n",
      "Epoch 58/200\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.3065 - val_loss: 0.3802\n",
      "Epoch 59/200\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3274 - val_loss: 0.3786\n",
      "Epoch 60/200\n",
      "363/363 [==============================] - 0s 549us/step - loss: 0.3109 - val_loss: 0.3805\n",
      "Epoch 61/200\n",
      "363/363 [==============================] - 0s 522us/step - loss: 0.3142 - val_loss: 0.3804\n",
      "Epoch 62/200\n",
      "363/363 [==============================] - 0s 542us/step - loss: 0.3268 - val_loss: 0.3802\n",
      "Epoch 63/200\n",
      "363/363 [==============================] - 0s 519us/step - loss: 0.3236 - val_loss: 0.3780\n",
      "Epoch 64/200\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.3093 - val_loss: 0.3785\n",
      "Epoch 65/200\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.3153 - val_loss: 0.3783\n",
      "Epoch 66/200\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.3213 - val_loss: 0.3764\n",
      "Epoch 67/200\n",
      "363/363 [==============================] - 0s 536us/step - loss: 0.3108 - val_loss: 0.3765\n",
      "Epoch 68/200\n",
      "363/363 [==============================] - 0s 536us/step - loss: 0.3199 - val_loss: 0.3770\n",
      "Epoch 69/200\n",
      "363/363 [==============================] - 0s 534us/step - loss: 0.3007 - val_loss: 0.3750\n",
      "Epoch 70/200\n",
      "363/363 [==============================] - 0s 531us/step - loss: 0.3243 - val_loss: 0.3748\n",
      "Epoch 71/200\n",
      "363/363 [==============================] - 0s 529us/step - loss: 0.3108 - val_loss: 0.3737\n",
      "Epoch 72/200\n",
      "363/363 [==============================] - 0s 539us/step - loss: 0.3201 - val_loss: 0.3737\n",
      "Epoch 73/200\n",
      "363/363 [==============================] - 0s 517us/step - loss: 0.3050 - val_loss: 0.3731\n",
      "Epoch 74/200\n",
      "363/363 [==============================] - 0s 533us/step - loss: 0.3147 - val_loss: 0.3751\n",
      "Epoch 75/200\n",
      "363/363 [==============================] - 0s 522us/step - loss: 0.3189 - val_loss: 0.3735\n",
      "Epoch 76/200\n",
      "363/363 [==============================] - 0s 522us/step - loss: 0.3192 - val_loss: 0.3730\n",
      "Epoch 77/200\n",
      "363/363 [==============================] - 0s 531us/step - loss: 0.3048 - val_loss: 0.3733\n",
      "Epoch 78/200\n",
      "363/363 [==============================] - 0s 531us/step - loss: 0.3236 - val_loss: 0.3716\n",
      "Epoch 79/200\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3167 - val_loss: 0.3701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "363/363 [==============================] - 0s 536us/step - loss: 0.3001 - val_loss: 0.3703\n",
      "Epoch 81/200\n",
      "363/363 [==============================] - 0s 543us/step - loss: 0.3160 - val_loss: 0.3734\n",
      "Epoch 82/200\n",
      "363/363 [==============================] - 0s 535us/step - loss: 0.3113 - val_loss: 0.3699\n",
      "Epoch 83/200\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.3113 - val_loss: 0.3722\n",
      "Epoch 84/200\n",
      "363/363 [==============================] - 0s 542us/step - loss: 0.3031 - val_loss: 0.3715\n",
      "Epoch 85/200\n",
      "363/363 [==============================] - 0s 589us/step - loss: 0.3071 - val_loss: 0.3696\n",
      "Epoch 86/200\n",
      "363/363 [==============================] - 0s 531us/step - loss: 0.3164 - val_loss: 0.3704\n",
      "Epoch 87/200\n",
      "363/363 [==============================] - 0s 536us/step - loss: 0.3142 - val_loss: 0.3702\n",
      "Epoch 88/200\n",
      "363/363 [==============================] - 0s 522us/step - loss: 0.3100 - val_loss: 0.3692\n",
      "Epoch 89/200\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.3164 - val_loss: 0.3682\n",
      "Epoch 90/200\n",
      "363/363 [==============================] - 0s 555us/step - loss: 0.3091 - val_loss: 0.3680\n",
      "Epoch 91/200\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.3035 - val_loss: 0.3679\n",
      "Epoch 92/200\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.3020 - val_loss: 0.3674\n",
      "Epoch 93/200\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.3128 - val_loss: 0.3675\n",
      "Epoch 94/200\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.3083 - val_loss: 0.3692\n",
      "Epoch 95/200\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.3071 - val_loss: 0.3678\n",
      "Epoch 96/200\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.3019 - val_loss: 0.3674\n",
      "Epoch 97/200\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.2959 - val_loss: 0.3668\n",
      "Epoch 98/200\n",
      "363/363 [==============================] - 0s 545us/step - loss: 0.3067 - val_loss: 0.3654\n",
      "Epoch 99/200\n",
      "363/363 [==============================] - 0s 543us/step - loss: 0.3098 - val_loss: 0.3649\n",
      "Epoch 100/200\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.2973 - val_loss: 0.3662\n",
      "Epoch 101/200\n",
      "363/363 [==============================] - 0s 589us/step - loss: 0.3208 - val_loss: 0.3654\n",
      "Epoch 102/200\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.2982 - val_loss: 0.3648\n",
      "Epoch 103/200\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.3199 - val_loss: 0.3659\n",
      "Epoch 104/200\n",
      "363/363 [==============================] - 0s 545us/step - loss: 0.3105 - val_loss: 0.3643\n",
      "Epoch 105/200\n",
      "363/363 [==============================] - 0s 561us/step - loss: 0.3228 - val_loss: 0.3641\n",
      "Epoch 106/200\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3183 - val_loss: 0.3648\n",
      "Epoch 107/200\n",
      "363/363 [==============================] - 0s 586us/step - loss: 0.3051 - val_loss: 0.3635\n",
      "Epoch 108/200\n",
      "363/363 [==============================] - 0s 615us/step - loss: 0.3063 - val_loss: 0.3633\n",
      "Epoch 109/200\n",
      "363/363 [==============================] - 0s 541us/step - loss: 0.2965 - val_loss: 0.3624\n",
      "Epoch 110/200\n",
      "363/363 [==============================] - 0s 540us/step - loss: 0.2977 - val_loss: 0.3630\n",
      "Epoch 111/200\n",
      "363/363 [==============================] - 0s 552us/step - loss: 0.3019 - val_loss: 0.3622\n",
      "Epoch 112/200\n",
      "363/363 [==============================] - 0s 538us/step - loss: 0.3013 - val_loss: 0.3619\n",
      "Epoch 113/200\n",
      "363/363 [==============================] - 0s 519us/step - loss: 0.3095 - val_loss: 0.3623\n",
      "Epoch 114/200\n",
      "363/363 [==============================] - 0s 528us/step - loss: 0.2907 - val_loss: 0.3619\n",
      "Epoch 115/200\n",
      "363/363 [==============================] - 0s 531us/step - loss: 0.3030 - val_loss: 0.3625\n",
      "Epoch 116/200\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.2900 - val_loss: 0.3611\n",
      "Epoch 117/200\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.2970 - val_loss: 0.3610\n",
      "Epoch 118/200\n",
      "363/363 [==============================] - 0s 528us/step - loss: 0.3077 - val_loss: 0.3612\n",
      "Epoch 119/200\n",
      "363/363 [==============================] - 0s 528us/step - loss: 0.3031 - val_loss: 0.3611\n",
      "Epoch 120/200\n",
      "363/363 [==============================] - 0s 542us/step - loss: 0.3017 - val_loss: 0.3600\n",
      "Epoch 121/200\n",
      "363/363 [==============================] - 0s 528us/step - loss: 0.2974 - val_loss: 0.3599\n",
      "Epoch 122/200\n",
      "363/363 [==============================] - 0s 519us/step - loss: 0.3026 - val_loss: 0.3600\n",
      "Epoch 123/200\n",
      "363/363 [==============================] - 0s 539us/step - loss: 0.3030 - val_loss: 0.3597\n",
      "Epoch 124/200\n",
      "363/363 [==============================] - 0s 533us/step - loss: 0.3015 - val_loss: 0.3597\n",
      "Epoch 125/200\n",
      "363/363 [==============================] - 0s 609us/step - loss: 0.3017 - val_loss: 0.3587\n",
      "Epoch 126/200\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.2881 - val_loss: 0.3601\n",
      "Epoch 127/200\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.2981 - val_loss: 0.3588\n",
      "Epoch 128/200\n",
      "363/363 [==============================] - 0s 605us/step - loss: 0.3052 - val_loss: 0.3592\n",
      "Epoch 129/200\n",
      "363/363 [==============================] - 0s 613us/step - loss: 0.2959 - val_loss: 0.3582\n",
      "Epoch 130/200\n",
      "363/363 [==============================] - 0s 648us/step - loss: 0.2943 - val_loss: 0.3591\n",
      "Epoch 131/200\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.3013 - val_loss: 0.3579\n",
      "Epoch 132/200\n",
      "363/363 [==============================] - 0s 552us/step - loss: 0.3038 - val_loss: 0.3597\n",
      "Epoch 133/200\n",
      "363/363 [==============================] - 0s 555us/step - loss: 0.3015 - val_loss: 0.3573\n",
      "Epoch 134/200\n",
      "363/363 [==============================] - 0s 528us/step - loss: 0.2897 - val_loss: 0.3571\n",
      "Epoch 135/200\n",
      "363/363 [==============================] - 0s 547us/step - loss: 0.3039 - val_loss: 0.3579\n",
      "Epoch 136/200\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.2856 - val_loss: 0.3561\n",
      "Epoch 137/200\n",
      "363/363 [==============================] - 0s 536us/step - loss: 0.2955 - val_loss: 0.3557\n",
      "Epoch 138/200\n",
      "363/363 [==============================] - 0s 528us/step - loss: 0.2898 - val_loss: 0.3561\n",
      "Epoch 139/200\n",
      "363/363 [==============================] - 0s 540us/step - loss: 0.3078 - val_loss: 0.3557\n",
      "Epoch 140/200\n",
      "363/363 [==============================] - 0s 535us/step - loss: 0.2931 - val_loss: 0.3549\n",
      "Epoch 141/200\n",
      "363/363 [==============================] - 0s 532us/step - loss: 0.3096 - val_loss: 0.3545\n",
      "Epoch 142/200\n",
      "363/363 [==============================] - 0s 534us/step - loss: 0.2823 - val_loss: 0.3551\n",
      "Epoch 143/200\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3077 - val_loss: 0.3559\n",
      "Epoch 144/200\n",
      "363/363 [==============================] - 0s 540us/step - loss: 0.2890 - val_loss: 0.3538\n",
      "Epoch 145/200\n",
      "363/363 [==============================] - 0s 534us/step - loss: 0.3016 - val_loss: 0.3539\n",
      "Epoch 146/200\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.2944 - val_loss: 0.3540\n",
      "Epoch 147/200\n",
      "363/363 [==============================] - 0s 553us/step - loss: 0.3071 - val_loss: 0.3540\n",
      "Epoch 148/200\n",
      "363/363 [==============================] - 0s 596us/step - loss: 0.2875 - val_loss: 0.3548\n",
      "Epoch 149/200\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.2896 - val_loss: 0.3530\n",
      "Epoch 150/200\n",
      "363/363 [==============================] - 0s 582us/step - loss: 0.2944 - val_loss: 0.3531\n",
      "Epoch 151/200\n",
      "363/363 [==============================] - 0s 542us/step - loss: 0.2850 - val_loss: 0.3534\n",
      "Epoch 152/200\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.3084 - val_loss: 0.3540\n",
      "Epoch 153/200\n",
      "363/363 [==============================] - 0s 589us/step - loss: 0.2945 - val_loss: 0.3523\n",
      "Epoch 154/200\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.2894 - val_loss: 0.3522\n",
      "Epoch 155/200\n",
      "363/363 [==============================] - 0s 555us/step - loss: 0.2918 - val_loss: 0.3534\n",
      "Epoch 156/200\n",
      "363/363 [==============================] - 0s 536us/step - loss: 0.2889 - val_loss: 0.3515\n",
      "Epoch 157/200\n",
      "363/363 [==============================] - 0s 561us/step - loss: 0.3012 - val_loss: 0.3514\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 594us/step - loss: 0.2852 - val_loss: 0.3529\n",
      "Epoch 159/200\n",
      "363/363 [==============================] - 0s 561us/step - loss: 0.2959 - val_loss: 0.3514\n",
      "Epoch 160/200\n",
      "363/363 [==============================] - 0s 539us/step - loss: 0.2912 - val_loss: 0.3510\n",
      "Epoch 161/200\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.2885 - val_loss: 0.3510\n",
      "Epoch 162/200\n",
      "363/363 [==============================] - 0s 586us/step - loss: 0.2818 - val_loss: 0.3509\n",
      "Epoch 163/200\n",
      "363/363 [==============================] - 0s 531us/step - loss: 0.2853 - val_loss: 0.3505\n",
      "Epoch 164/200\n",
      "363/363 [==============================] - 0s 522us/step - loss: 0.2825 - val_loss: 0.3521\n",
      "Epoch 165/200\n",
      "363/363 [==============================] - 0s 517us/step - loss: 0.2937 - val_loss: 0.3509\n",
      "Epoch 166/200\n",
      "363/363 [==============================] - 0s 522us/step - loss: 0.2841 - val_loss: 0.3511\n",
      "Epoch 167/200\n",
      "363/363 [==============================] - 0s 526us/step - loss: 0.2903 - val_loss: 0.3509\n",
      "Epoch 168/200\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.2884 - val_loss: 0.3511\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", \n",
    "                                               save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5,\n",
    "                                                 restore_best_weights=True)\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "hist = model.fit(x_train, y_train, epochs=200,\n",
    "                  validation_data=(x_val, y_val),\n",
    "                callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "automatic-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000+1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step/10), step=step)\n",
    "        \n",
    "        data = (np.random.randn(100)+2) * step/100\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "        \n",
    "        images = np.random.rand(2, 32, 32, 3)\n",
    "        tf.summary.image(\"my_images\", images*step/1000, step=step)\n",
    "        \n",
    "        texts = [\"The step is \" + str(step), \"Its square is \", str(step**2)]\n",
    "        tf.summary.text(\"my_text\", texts, step=step)\n",
    "        \n",
    "        sine_wave = tf.math.sin(tf.range(12000)/48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio(\"my_audio\", audio, sample_rate=24000, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-lincoln",
   "metadata": {},
   "source": [
    "**tf.summary** 패키지를 이용하여 저수준에서 텐서보드 로그를 만드는 방법.  \n",
    "스칼라, 히스토그램, 이미지, 오디오, 텍스트 등 여러가지 정보를 로그로 만들 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "norwegian-action",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.45020635e+00, -1.68286334e+00, -8.74575628e-01, -9.32650703e-01,\n",
       "       -1.75834461e-01, -6.26521368e-01,  1.33874202e+00,  1.51790038e+00,\n",
       "       -6.61331878e-01, -2.60644333e-01, -7.97762275e-01,  2.29858851e-03,\n",
       "        4.09361961e-01, -5.56289943e-01, -5.69480478e-01, -9.06457907e-01,\n",
       "        5.40825196e-03, -1.92204382e+00,  5.63634239e-01, -7.15341513e-02,\n",
       "       -7.07368860e-01,  1.94188080e+00, -1.00360124e-01,  9.04700066e-01,\n",
       "       -1.11180206e+00,  1.88480607e-01,  2.30865430e-01, -8.59603288e-01,\n",
       "        5.26812150e-01,  6.11759908e-01, -4.07466879e-01,  8.82322750e-01,\n",
       "        6.72408317e-01, -2.50126261e-01, -4.91759691e-01,  1.30303009e+00,\n",
       "       -2.06822843e-01, -8.50266819e-02, -3.74476851e-01, -3.12461552e-02,\n",
       "       -3.99829039e-01, -7.01129160e-01,  1.48378973e+00,  6.11516304e-01,\n",
       "       -3.21657506e-01,  1.00909510e+00, -6.21692627e-01,  4.82294205e-01,\n",
       "       -6.70475780e-01,  7.96829017e-01, -9.47730628e-01, -3.72041056e-01,\n",
       "        7.82253820e-01,  1.14715852e+00, -1.07749243e+00,  1.51709508e+00,\n",
       "        8.73748233e-01, -9.74616325e-01, -4.65242898e-01, -1.43962014e-01,\n",
       "        9.75370050e-01, -6.71186914e-01,  2.29167888e+00,  1.41457213e+00,\n",
       "       -6.75003498e-01,  7.40387400e-02,  4.83334169e-01,  2.88602977e-01,\n",
       "        1.40126407e+00,  1.88435064e+00, -1.75249344e+00, -1.06667924e+00,\n",
       "       -7.51344978e-01,  2.97885045e+00, -4.96915629e-01,  1.34122629e+00,\n",
       "        6.29190282e-01,  2.96100988e-01,  7.94329711e-01, -1.19794308e+00,\n",
       "       -2.04487908e-01,  1.08871208e+00, -2.19399125e-01,  1.64634627e+00,\n",
       "        9.95979779e-01, -1.63482840e+00, -2.61810099e-01, -7.15958908e-02,\n",
       "        1.25095574e+00,  4.18140952e-01,  8.73320999e-01,  7.46846776e-01,\n",
       "       -1.05521422e+00, -1.33511794e+00,  8.46448024e-02,  1.26602171e+00,\n",
       "       -3.15102219e-01,  2.17656924e-01, -2.93697703e-01,  4.87456310e-01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-retreat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
