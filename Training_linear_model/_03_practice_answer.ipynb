{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 수백만 개의 특성을 가진 학습데이터셋에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까요?\n",
    "메모리가 충분하다면 경사 하강법, 모자라다면 미니배치 또는 확률적 경사 하강법.  \n",
    "정규방정식이나 특잇값 분해는 계산 복잡도가 특성 개수에 따라서 매우 크게 증가하기 때문에 사용하기 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 학습데이터셋에 있는 특성들이 각기 다른 스케일을 갖고 있다. 이런 데이터에 잘 작동하지 않는 알고리즘은 무엇일까요? 그 이유는 무엇일까요? 이 문제를 어떻게 해결할까요?\n",
    "학습 데이터셋의 특성의 스케일이 매우 다르면 비용 함수가 길쭉한 타원 모양이 됨. 이것은 경사 하강법의 수렴 속도를 늦춤.  \n",
    "이를 해결하기 위해선 각 특성을 스케일링해야함. 정규방정식이나 특잇값 분해는 스케일에 영향을 받지 않음.  \n",
    "또한 규제가 있는 모델은 특성의 스케일이 다를 경우 로컬 미니멈에 빠질 수도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. 경사 하강법으로 로지스틱 회귀 모델을 학습할 때 로칼 미니멈에 빠질 가능성이 있을까요?\n",
    "로지스틱 회귀의 비용 함수는 볼록함수라서 로컬 미니멈이 존재하지 않음. 따라서 빠질 가능성 없음\n",
    "\n",
    "\n",
    "---\n",
    "## 4. 충분히 오랫동안 실행하면 모든 경사 하강법 알고리즘이 같은 모델을 만들까요?\n",
    "최적화할 비용함수가 볼록함수이고 학습률이 너무 크지만 않다면 모든 경사 하강법 알고리즘은 글로벌 미니멈에 도달할 수 있음.  \n",
    "하지만 학습률을 점진적으로 감소시키지 않으면 미니배치, 확률적 경사 하강법은 최적점 근처에서 진동할 뿐 수렴은 못함.  \n",
    "\n",
    "---\n",
    "## 5. 배치 경사 하강법을 사용하고 에폭마다 검증 오차를 그래프로 나타냈다. 검증 오차가 일정하게 상승하고 있다면 어떤 일이 일어나고 있는 걸까요? 이 문제를 어떻게 해결할까요?\n",
    "학습 오차와 검증 오차 둘 다 동시에 오르면 **발산** : 학습률 감소  \n",
    "학습 오차는 줄고 검증 오차만 오르면 **과대적합** : 학습 중단  \n",
    "\n",
    "---\n",
    "## 6. 검증 오차가 상승하면 미니 배치 경사 하강법을 즉시 중단하는 것이 좋은 방법인가?\n",
    "미니배치, 확률적 경사 하강법은 무작위성 때문에 매 학습마다 진전을 보장하지 못함.  \n",
    "정기적으로 모델을 저장하고 오랫동안 진전이 없을 때 저장된 것 중 가장 좋은 모델을 사용하는 것이 좋음\n",
    "\n",
    "---\n",
    "## 7. 배치, 미니배치, 확률적 경사 하강법 중에 가장 빠르게 최적 솔루션의 주변에 도달할까요? 실제로 수렴하는 것은 어떤 것인가요? 다른 방법들도 수렴하게 만들 수 있나요?\n",
    "\n",
    "---\n",
    "## 8. 다항 회귀를 사용했을 때 학습 곡선을 보니 훈련 오차와 검증 오차 사이에 간격이 크다. 무슨 일이 생긴건가요? 이 문제를 해결할 3가지 방법은 무엇인가요?\n",
    "과대적합되었을 가능성이 큼. \n",
    "- 다항 차수 줄이기  \n",
    "- 모델 규제하기(비용 함수에 릿지패널티, 라쏘패널티 등 추가)  \n",
    "- 학습 데이터셋 크기 증가\n",
    "\n",
    "---\n",
    "## 9. 릿지 회귀를 사용했을 때 훈련 오차와 검증 오차가 거의 비슷하고 둘 다 높았다. 이 모델에는 높은 편향이 문제일까? 아니면 높은 분산이 문제일까? 규제 하이퍼파라이머 알파를 증가시켜야 할까? 감소시켜야 할까?\n",
    "높은 편향이 문제임. 따라서 규제 하이어파라미터 알파를 감소시켜야 함.\n",
    "\n",
    "---\n",
    "## 10. 다음과 같이 사용해야 하는 이유는?\n",
    "- **평범한 선형 회귀(아무런 규제가 없는) 대신 릿지 회귀** : 일반적으로 규제가 있는 모델이 더 좋아서  \n",
    "- **릿지 회귀 대신 라쏘 회귀** : 라쏘 회귀는 중요하지 않은 가중치를 0으로 만드는 특성이 있음. 이는 특성 선택의 효과를 가지므로 몇 개의 특성만 실제로 유용할 것이라고 의심될 때 사용하면 좋음. 확신이 없으면 릿지 회귀 사용하기  \n",
    "- **라쏘 회귀 대신 엘라스틱넷** : 라쏘가 어떤 경우에(몇 개의 특성이 강하게 연관되어 있거나 학습 햄플보다 특성이 더 많을 때)에는 불규칙하게 동작하므로 엘라스틱넷이 일반적으로 라쏘보다 선호됨. \n",
    "\n",
    "---\n",
    "## 11. 사진을 낮과 밤, 실내와 실외로 분류하려고 함. 두 로지스틱 회귀 분류기를 만들어야 할까? 아니면 하나의 소프트맥스 회귀 분류를 만들어야 할까?\n",
    "이 둘은 배타적인 클래스가 아님. 즉 둘이 동시에 일어날 수 있기 때문에 두 개의 로지스틱 회귀 분류기를 따로 학습해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
