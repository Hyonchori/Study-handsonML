{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "funded-worship",
   "metadata": {},
   "source": [
    "# 연습 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-service",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. 이미지 분류에서 풀리 커넥티드 DNN 보다 CNN이 나은 점은?\n",
    "- 연속된 층이 부분적으로 연결되어 있고 많은 가중치를 재사용하기 때문에 CNN이 **훨씬 적은 파라미터를 가짐**. 따라서 학습 속도가 빠르고 과대 적합의 위험을 줄이며 더 적은 데이터를 필요로 함.\n",
    "- CNN이 어떤 특성을 감지할 수 있는 커널을 학습하면 이미지의 어느 위치에 있는 특성이라도 감지할 수 있음. 반면에 DNN은 한 위치에 있는 특성을 학습하면 특정 위치에 있는 것만 감지할 수 있음. 따라서 CNN이 **더 높은 일반화 성능을 낼 수 있음**\n",
    "- DNN은 픽셀이 어떻게 조직되어 있는지 모름. CNN의 하위층은 일반적으로 이미지의 작은 영역에 있는 특성을 구별하고, 상위층은 저수준 특성을 더 큰 특성으로 연결함. **CNN이 대부분의 자연적인 이미지에 더 유리함**\n",
    "\n",
    "\n",
    "---\n",
    "## 2. 3x3 커널, 스트라이드 2, \"same\" 패딩으로 된 합성곱 층 세 개로 구성된 CNN이 있다. 가장 아래 층은 특성 맵 100개를 출력하고, 중간 층은 200개, 가장 위의 층은 400개를 출력함. \n",
    "\n",
    "### 2.1 입력 이미지는 200x200일 때, 이 CNN의 전체 파라미터 수는??\n",
    "\n",
    "\n",
    "\n",
    "### 2.2 32비트 부동 소수를 사용할 때, 하나의 샘플을 예측하기 위해 필요한 RAM은?\n",
    "\n",
    "\n",
    "### 2.3 50개의 이미지를 미니 배치로 학습할 때 필요한 RAM은?\n",
    "\n",
    "\n",
    "---\n",
    "## 3.  어떤 CNN을 학습할 때 GPU 메모리 부족이 발생했다면 시도해볼 수 있는 다섯 가지는?\n",
    "1. 미니 배치 크기 줄이기\n",
    "2. 하나 이상의 층에서 스트라이드를 크게 하여 차원을 감소시키기\n",
    "3. 하나 이상의 층을 제거하기\n",
    "4. 32비트 부동소구 대신 16비트 부동소수 사용하기\n",
    "5. 여러 장치에 CNN을 분산하기\n",
    "\n",
    "\n",
    "---\n",
    "## 4. 같은 크기의 스트라이드의 합성곱 층 대신 최대 풀링층을 추가하는 이유는?\n",
    "최대 풀링층은 파라미터를 전혀 갖고 있지 않지만 합성곱층은 상당한 양의 파라미터를 가지기 때문\n",
    "\n",
    "\n",
    "---\n",
    "## 5. LRN 층을 추가해야 할 때는 언제인가?\n",
    "LRN층은 가장 강하게 활성화되는 뉴런이 이웃한 특성 맵의 동일한 위치에 있는 뉴런을 억제함.  \n",
    "따라서 특성 맵마다 특별하고 구분되게 만들어 넓은 범위의 특성을 탐색하도록 강제함.  \n",
    "상위 층에서 필요한 저수준 특성을 많이 찾기 위해 일반적으로 하위층에서 사용.\n",
    "\n",
    "\n",
    "---\n",
    "## 6. LeNet-5 와 비교해서 AlexNet 의 혁신점은? GoogleNet, ResNet, SENet, Xception 의 혁신점은?\n",
    "> **AlexNet의 혁신**  \n",
    ">- 더 크고 깊으며\n",
    ">- 합성곱층 위에 풀링층을 두지 않고 합성곱층으로만 직접 쌓아 올림.\n",
    "\n",
    "> **GoogleNet의 혁신**  \n",
    "> - 더 적은 파라미터로 종전의 CNN 구조보다 더 깊은 신경망을 만들 수 있도록 **인셉션 모듈**을 고안.\n",
    "\n",
    "> **ResNet**\n",
    ">- 100개 이상 층의 신경망을 구성할 수 있도록 **스킵 연결**을 만듦.\n",
    "\n",
    ">**SENet**\n",
    ">- 인셉션 네트워크에 있는 인셉션 모듈, 또는 ResNet에 있는 잔차 유닛 다음에 **SE 블록(밀집 층 두 개로 구성된 네트워크)** 을 사용하여 특성 맵의 상대적 중요도를 보정한 것.\n",
    "\n",
    ">**Xception**\n",
    ">- 공간 패턴과 깊이별 패턴을 나누어보는 **깊이별 분리 합성곱**을 사용\n",
    "\n",
    "\n",
    "---\n",
    "## 7. 완전 합성곱 신경망이란 무엇인가? 밀집 층을 어떻게 합성곱 층으로 바꿀 수 있는가?\n",
    "**합성곱과 풀링층으로만 구성된 신경망.**  \n",
    "- FCN은 어떤 크기의 너비와 높이를 가진 이미지라도 효율적으로 처리할 수 있음.  \n",
    "- 이미지를 딱 한 번만 보기 때문에 **객체 탐지와 시맨틱 분할에 유용함.**\n",
    "- 몇 개의 밀집 층이 위에 놓인 CNN이 있다면 밀집 층을 합성곱 층으로 바꾸어 FCN으로 바꿀 수 있음.\n",
    ">- 이 합성곱 층의 커널 크기는 가장 아래쪽 밀집 층의 입력 크기와 같고\n",
    ">- 필터 개수는 밀집 층의 뉴런 개수와 같고 **valid** 패딩을 사용\n",
    ">- 일반적으로 스트라이드는 1이지만 필요하면 더 큰 수로 지정할 수 있음.\n",
    ">- 활성함수는 밀집 층과 같은 활성함수 사용\n",
    ">- 다른 밀집층도 같은 방식으로 변환하지만 **1x1** 필터를 사용.\n",
    "\n",
    "---\n",
    "## 8. 시맨틱 분할에서 주요한 기술적 어려움은?\n",
    "\n",
    "CNN에서 신호가 층을 거쳐 전달되면서 **공간상의 정보가 많이 사라진다는 것**  \n",
    "특히 풀링 층과 스트라이드가 1보다 큰 층에서 발생.  \n",
    "이런 공간상의 정보는 각 픽셀의 클래스를 정확히 예측하기 위해 복원하는 데 필요함.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-termination",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
