{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 부스팅\n",
    "부스팅은 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법이다.  \n",
    "앞의 모델을 보완해 나가면서 일련의 예측기를 학습시키는 것임.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 에이다부스팅\n",
    "**Adaptive boosting** . 가장 인기있는 부스팅 방법.  \n",
    "> 1. 알고리즘의 기반이 되는 첫 번째 분류기를 학습 데이터셋에 학습 후 예측  \n",
    "2. 알고리즘이 잘못 분류한 샘플의 가중치를 상대적으로 높임.  \n",
    "3. 업데이트된 가중치를 사용하여 학습 데이터셋에 다시 학습 후 예측.  \n",
    "4. 위 과정 반복  \n",
    "  \n",
    "> 모든 예측기가 학습을 마치면 배깅이나 페이스팅과 비슷한 방식으로 최종 예측을 만듦.  \n",
    "그런데 이 방법은 각 예측기가 이전 예측기가 학습하고 평가한 후에야 학습할 수 있기 때문에 **병렬화를 할 수 없음**  \n",
    "따라서 배깅이나 페이스팅보다 확장성은 떨어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_estimators**와 **learning_rate**는 트레이드오프관계라고 함.(왜..??)  \n",
    "**algorithm** 은 기본적으로 저걸 쓰는 듯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 그래디언트 부스팅\n",
    "**Gradient boosting** . 에이다부스트처럼 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가하는 것.  \n",
    "그래디언트부스팅은 샘플의 가중치를 수정하는 대신에 **이전 예측기가 만든 잔여오차에 새로운 예측기를 학습함**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결정 트리를 기반 예측기로 사용하는 그래디언트 부스팅 방법이 많이 쓰임  \n",
    "이를 **그래디언트 트리 부스팅**, 또는 **그래디언티드 부스티드 회귀 트리(GBRT)** 라고 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GradientBoostingRegressor** 클래스로 GBRT를 구현할 수 있음. 랜덤포레스트처럼 트리의 성장을 제어하는 매개변수를 갖고 있음.  \n",
    "**learning_rate** 매개변수는 각 트리의 기여 정도를 조절함. 0.1처럼 낮게 설정하면 앙상블예측기는 학습데이터를 학습하기 위해 많은 트리가 필요하지만 일반적으로 예측의 성능은 좋아짐. (아, 그래서 서로 트레이드오프 관계라는 거)  \n",
    "> learning_rate크고 n_estimator작을수록 : 모델 과소적합?  \n",
    "learning_rate작고 n_estimator클수록 : 모델 과대적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "x, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125,)\n",
      "(125,)\n",
      "(125,)\n"
     ]
    }
   ],
   "source": [
    "for pred in gbrt.staged_predict(x_test):\n",
    "    print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**staged_predict** 메서드로 각 예측기의 예측값을 순서대로 출력할 수도 있음.  \n",
    "가장 오차가 적은 예측기가 몇 번째인지를 판단하는 것으로 **최적의 트리 수** 를 추정할 수도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 확률적 그래디언트 부스팅\n",
    "GradientBoostingRegressor에서 각 트리가 훈련할 때 사용할 훈련 샘플의 비율을 지정할 수 있음.  \n",
    "**subsample** 매개변수를 이용하면 됨. 예를 들어 subsample=0.25일 경우 각 트리는 무작위로 선택된 25%의 학습 데이터로 학습함.  \n",
    "역시 편향이 높아지는 대신 분산이 낮아짐.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적화된 그래디언트 부스팅 구현으로 **XGBoost 라이브러리**가 유명함  \n",
    "매우 빠른 속도, 확장성, 이식성이 장점임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08984928005708914"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = xgb_reg.predict(x_test)\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
