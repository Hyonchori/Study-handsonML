{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "distinguished-charger",
   "metadata": {},
   "source": [
    "# 긴 시퀀스 다루기\n",
    "긴 시퀀스로 기본 RNN을 학습하려면 많은 타임 스텝에 걸쳐 실행해야 하므로 펼친 RNN이 매우 깊은 네트워크가 됨.  \n",
    "보통의 심층 신경망처럼 \n",
    "- 그레디언트 소실과 폭주 문제를 가질 수 있음. 학습하는 데 시간이 오래 걸리고 불안정할 수 있음.  \n",
    "- 또한 RNN이 긴 시퀀스를 처리할 때 입력의 첫 부분을 조금씩 잊어버릴 것임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-virtue",
   "metadata": {},
   "source": [
    "---\n",
    "## 불안정한 그레디언트 문제\n",
    "이전에 심층 신경망에서 사용했던 기법들\n",
    "- 좋은 가중치 초기화 및 활성 함수\n",
    "- 빠른 옵티마이저\n",
    "- 드롭아웃 등\n",
    "\n",
    "**하지만 RNN에선 수렴하지 않은 활성 함수(ReLU같은)는 학습할 때 더 불안정하게 만든다고 함.** 이는 그레디언트 폭주를 일으킬 수 있다고 함.  \n",
    "그래서 **하이퍼볼릭 탄젠트**같은 함수를 많이 사용한다고 함.  \n",
    "> 그래도 그레디언트가 폭주하면 **그레디언트 클리핑** 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-adjustment",
   "metadata": {},
   "source": [
    "**배치 정규화도 RNN에 효율적으로 사용할 수는 없다고 함**. 타임 스텝 사이에는 사용할 수 없고 층 사이에만 가능함.  \n",
    "대신 **층 정규화 (layer normalization)** 이 잘 맞다고 함.\n",
    "> 배치 정규화와 비슷하지만 배치 차원에 대해 정규화하는 대신에 **특성 차원에 대해 정규화함.**  \n",
    "샘플에 독립적으로 타임 스텝마다 동적으로 필요한 통계를 계산하고 이는 학습과 테스트에서 동일한 방식으로 작동한다는 것을 의미.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "metric-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가상 장치가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "consolidated-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNSimpleRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None)\n",
    "        self.layer_norm = keras.layers.LayerNormalization()\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    \n",
    "    def call(self, inputs, states):\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-terrain",
   "metadata": {},
   "source": [
    "**층 정규화를 적용한 RNN cell 예시**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "parental-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-affect",
   "metadata": {},
   "source": [
    "---\n",
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "strange-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.LSTM(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caroline-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offset1, offset2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5*np.sin((time - offset1)*(freq1*10 + 10))\n",
    "    series += 0.2*np.sin((time - offset2)*(freq2*20 + 20))\n",
    "    series += (np.random.rand(batch_size, n_steps) - 0.5)\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "express-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "\n",
    "series = generate_time_series(10000, n_steps+10)\n",
    "X_train, Y_train = series[: 7000, :n_steps], series[: 7000, -10:, 0]\n",
    "X_valid, Y_valid = series[7000: 9000, :n_steps], series[7000: 9000, -10:, 0]\n",
    "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]\n",
    "\n",
    "Y = np.empty((10000, n_steps, 10))\n",
    "for step_ahead in range(1, 10+1):\n",
    "    Y[:, :, step_ahead-1] = series[:, step_ahead:step_ahead+n_steps, 0]\n",
    "\n",
    "Y_train = Y[: 7000]\n",
    "Y_valid = Y[7000: 9000]\n",
    "Y_test = Y[9000: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tribal-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_time_step_mse(Y_true, Y_pred):\n",
    "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "lstm_model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "documentary-interest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "28/28 [==============================] - 9s 272ms/step - loss: 0.5698 - last_time_step_mse: 0.5685 - val_loss: 0.2936 - val_last_time_step_mse: 0.2927\n",
      "Epoch 2/20\n",
      "28/28 [==============================] - 7s 254ms/step - loss: 0.2567 - last_time_step_mse: 0.2524 - val_loss: 0.1962 - val_last_time_step_mse: 0.1841\n",
      "Epoch 3/20\n",
      "28/28 [==============================] - 7s 260ms/step - loss: 0.1878 - last_time_step_mse: 0.1744 - val_loss: 0.1753 - val_last_time_step_mse: 0.1623\n",
      "Epoch 4/20\n",
      "28/28 [==============================] - 7s 255ms/step - loss: 0.1727 - last_time_step_mse: 0.1597 - val_loss: 0.1694 - val_last_time_step_mse: 0.1567\n",
      "Epoch 5/20\n",
      "28/28 [==============================] - 7s 255ms/step - loss: 0.1673 - last_time_step_mse: 0.1551 - val_loss: 0.1677 - val_last_time_step_mse: 0.1553\n",
      "Epoch 6/20\n",
      "28/28 [==============================] - 7s 258ms/step - loss: 0.1653 - last_time_step_mse: 0.1527 - val_loss: 0.1634 - val_last_time_step_mse: 0.1514\n",
      "Epoch 7/20\n",
      "28/28 [==============================] - 7s 263ms/step - loss: 0.1608 - last_time_step_mse: 0.1494 - val_loss: 0.1613 - val_last_time_step_mse: 0.1498\n",
      "Epoch 8/20\n",
      "28/28 [==============================] - 7s 266ms/step - loss: 0.1603 - last_time_step_mse: 0.1504 - val_loss: 0.1601 - val_last_time_step_mse: 0.1486\n",
      "Epoch 9/20\n",
      "28/28 [==============================] - 7s 265ms/step - loss: 0.1587 - last_time_step_mse: 0.1488 - val_loss: 0.1588 - val_last_time_step_mse: 0.1475\n",
      "Epoch 10/20\n",
      "28/28 [==============================] - 7s 263ms/step - loss: 0.1583 - last_time_step_mse: 0.1462 - val_loss: 0.1580 - val_last_time_step_mse: 0.1467\n",
      "Epoch 11/20\n",
      "28/28 [==============================] - 7s 255ms/step - loss: 0.1571 - last_time_step_mse: 0.1463 - val_loss: 0.1572 - val_last_time_step_mse: 0.1461\n",
      "Epoch 12/20\n",
      "28/28 [==============================] - 7s 258ms/step - loss: 0.1560 - last_time_step_mse: 0.1453 - val_loss: 0.1565 - val_last_time_step_mse: 0.1457\n",
      "Epoch 13/20\n",
      "28/28 [==============================] - 7s 253ms/step - loss: 0.1552 - last_time_step_mse: 0.1451 - val_loss: 0.1561 - val_last_time_step_mse: 0.1454\n",
      "Epoch 14/20\n",
      "28/28 [==============================] - 7s 263ms/step - loss: 0.1542 - last_time_step_mse: 0.1446 - val_loss: 0.1555 - val_last_time_step_mse: 0.1450\n",
      "Epoch 15/20\n",
      "28/28 [==============================] - 7s 263ms/step - loss: 0.1547 - last_time_step_mse: 0.1448 - val_loss: 0.1551 - val_last_time_step_mse: 0.1445\n",
      "Epoch 16/20\n",
      "28/28 [==============================] - 7s 266ms/step - loss: 0.1539 - last_time_step_mse: 0.1435 - val_loss: 0.1547 - val_last_time_step_mse: 0.1440\n",
      "Epoch 17/20\n",
      "28/28 [==============================] - 8s 269ms/step - loss: 0.1534 - last_time_step_mse: 0.1435 - val_loss: 0.1545 - val_last_time_step_mse: 0.1437\n",
      "Epoch 18/20\n",
      "28/28 [==============================] - 8s 270ms/step - loss: 0.1536 - last_time_step_mse: 0.1439 - val_loss: 0.1540 - val_last_time_step_mse: 0.1432\n",
      "Epoch 19/20\n",
      "28/28 [==============================] - 7s 252ms/step - loss: 0.1525 - last_time_step_mse: 0.1412 - val_loss: 0.1539 - val_last_time_step_mse: 0.1431\n",
      "Epoch 20/20\n",
      "28/28 [==============================] - 7s 259ms/step - loss: 0.1522 - last_time_step_mse: 0.1423 - val_loss: 0.1534 - val_last_time_step_mse: 0.1426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25311cae9a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "numerical-consensus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "28/28 [==============================] - 3s 29ms/step - loss: 0.2259 - last_time_step_mse: 0.2247 - val_loss: 0.2034 - val_last_time_step_mse: 0.1989\n",
      "Epoch 2/20\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.1921 - last_time_step_mse: 0.1847 - val_loss: 0.1720 - val_last_time_step_mse: 0.1629\n",
      "Epoch 3/20\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.1700 - last_time_step_mse: 0.1609 - val_loss: 0.1636 - val_last_time_step_mse: 0.1542\n",
      "Epoch 4/20\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.1623 - last_time_step_mse: 0.1521 - val_loss: 0.1588 - val_last_time_step_mse: 0.1498\n",
      "Epoch 5/20\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.1572 - last_time_step_mse: 0.1480 - val_loss: 0.1562 - val_last_time_step_mse: 0.1480\n",
      "Epoch 6/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1551 - last_time_step_mse: 0.1467 - val_loss: 0.1542 - val_last_time_step_mse: 0.1457\n",
      "Epoch 7/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1538 - last_time_step_mse: 0.1440 - val_loss: 0.1528 - val_last_time_step_mse: 0.1439\n",
      "Epoch 8/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1512 - last_time_step_mse: 0.1412 - val_loss: 0.1508 - val_last_time_step_mse: 0.1383\n",
      "Epoch 9/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1496 - last_time_step_mse: 0.1362 - val_loss: 0.1493 - val_last_time_step_mse: 0.1351\n",
      "Epoch 10/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1481 - last_time_step_mse: 0.1312 - val_loss: 0.1476 - val_last_time_step_mse: 0.1314\n",
      "Epoch 11/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1467 - last_time_step_mse: 0.1296 - val_loss: 0.1466 - val_last_time_step_mse: 0.1300\n",
      "Epoch 12/20\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.1448 - last_time_step_mse: 0.1270 - val_loss: 0.1453 - val_last_time_step_mse: 0.1272\n",
      "Epoch 13/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1445 - last_time_step_mse: 0.1271 - val_loss: 0.1446 - val_last_time_step_mse: 0.1265\n",
      "Epoch 14/20\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.1437 - last_time_step_mse: 0.1251 - val_loss: 0.1437 - val_last_time_step_mse: 0.1249\n",
      "Epoch 15/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1432 - last_time_step_mse: 0.1234 - val_loss: 0.1435 - val_last_time_step_mse: 0.1255\n",
      "Epoch 16/20\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.1428 - last_time_step_mse: 0.1245 - val_loss: 0.1426 - val_last_time_step_mse: 0.1233\n",
      "Epoch 17/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1418 - last_time_step_mse: 0.1232 - val_loss: 0.1421 - val_last_time_step_mse: 0.1228\n",
      "Epoch 18/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1413 - last_time_step_mse: 0.1228 - val_loss: 0.1417 - val_last_time_step_mse: 0.1228\n",
      "Epoch 19/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1408 - last_time_step_mse: 0.1228 - val_loss: 0.1413 - val_last_time_step_mse: 0.1220\n",
      "Epoch 20/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1409 - last_time_step_mse: 0.1214 - val_loss: 0.1411 - val_last_time_step_mse: 0.1223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x254294fbb80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid), batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-homeless",
   "metadata": {},
   "source": [
    ">오 LSTM이 학습 속도도 훨씬 빠르고 성능도 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-expansion",
   "metadata": {},
   "source": [
    "---\n",
    "## GRU\n",
    "**게이트 순환 유닛 (gated recurrent unit)** 은 LSTM의 변종으로 더 간소화되고 유사하게 작동함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sacred-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "polish-deposit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "28/28 [==============================] - 3s 26ms/step - loss: 0.2283 - last_time_step_mse: 0.2285 - val_loss: 0.2066 - val_last_time_step_mse: 0.2044\n",
      "Epoch 2/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1976 - last_time_step_mse: 0.1931 - val_loss: 0.1768 - val_last_time_step_mse: 0.1744\n",
      "Epoch 3/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1717 - last_time_step_mse: 0.1679 - val_loss: 0.1608 - val_last_time_step_mse: 0.1564\n",
      "Epoch 4/20\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1581 - last_time_step_mse: 0.1508 - val_loss: 0.1544 - val_last_time_step_mse: 0.1483\n",
      "Epoch 5/20\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1544 - last_time_step_mse: 0.1463 - val_loss: 0.1534 - val_last_time_step_mse: 0.1475\n",
      "Epoch 6/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1531 - last_time_step_mse: 0.1450 - val_loss: 0.1528 - val_last_time_step_mse: 0.1469\n",
      "Epoch 7/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1524 - last_time_step_mse: 0.1444 - val_loss: 0.1521 - val_last_time_step_mse: 0.1463\n",
      "Epoch 8/20\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1512 - last_time_step_mse: 0.1438 - val_loss: 0.1516 - val_last_time_step_mse: 0.1458\n",
      "Epoch 9/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1520 - last_time_step_mse: 0.1450 - val_loss: 0.1512 - val_last_time_step_mse: 0.1455\n",
      "Epoch 10/20\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1507 - last_time_step_mse: 0.1447 - val_loss: 0.1513 - val_last_time_step_mse: 0.1459\n",
      "Epoch 11/20\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1509 - last_time_step_mse: 0.1426 - val_loss: 0.1507 - val_last_time_step_mse: 0.1451\n",
      "Epoch 12/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1503 - last_time_step_mse: 0.1433 - val_loss: 0.1504 - val_last_time_step_mse: 0.1447\n",
      "Epoch 13/20\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1497 - last_time_step_mse: 0.1417 - val_loss: 0.1502 - val_last_time_step_mse: 0.1443\n",
      "Epoch 14/20\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1497 - last_time_step_mse: 0.1422 - val_loss: 0.1501 - val_last_time_step_mse: 0.1444\n",
      "Epoch 15/20\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1497 - last_time_step_mse: 0.1427 - val_loss: 0.1496 - val_last_time_step_mse: 0.1438\n",
      "Epoch 16/20\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1492 - last_time_step_mse: 0.1415 - val_loss: 0.1493 - val_last_time_step_mse: 0.1435\n",
      "Epoch 17/20\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1486 - last_time_step_mse: 0.1413 - val_loss: 0.1491 - val_last_time_step_mse: 0.1431\n",
      "Epoch 18/20\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1484 - last_time_step_mse: 0.1400 - val_loss: 0.1487 - val_last_time_step_mse: 0.1428\n",
      "Epoch 19/20\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1485 - last_time_step_mse: 0.1414 - val_loss: 0.1485 - val_last_time_step_mse: 0.1426\n",
      "Epoch 20/20\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1483 - last_time_step_mse: 0.1403 - val_loss: 0.1482 - val_last_time_step_mse: 0.1422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25430f697c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "gru_model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid), batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-electronics",
   "metadata": {},
   "source": [
    ">LSTM 보다 훨씬 학습이 빠르게 됨.  \n",
    "성능 자체는 LSTM이 더 좋은듯?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-moldova",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
